[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on_Ex01",
    "section": "",
    "text": "importing polygon feature data in shapefile format\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. Note that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed.\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "importing geospatial data into R\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\Hands-on_Ex\\Hands-on_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\nData Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\nYOUNG: age group 0 to 4 until age group 20 to 24, ECONOMY ACTIVE: age group 25-29 until age group 60-64, AGED: age group 65 and above, TOTAL: all age group, and DEPENDENCY: the ratio between young and aged against economy active group\n\n\nImporting data in R\n\nData Wrangling\n\npopdata <- read.csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\nJoining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\nJoining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\nChoropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\nPlotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\nCreating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\nDrawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\nDrawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\nDrawing a choropleth map using tm_fill() and tm_border()\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\nPlotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method."
  },
  {
    "objectID": "In-class_Ex/In-class_ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_ex02/In-class_Ex02.html",
    "title": "In-class_Ex02",
    "section": "",
    "text": "Show the code\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex02/In-class_Ex02.html#importing-geospatial",
    "href": "In-class_Ex/In-class_ex02/In-class_Ex02.html#importing-geospatial",
    "title": "In-class_Ex02",
    "section": "3.1 Importing Geospatial",
    "text": "3.1 Importing Geospatial\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the steps you had learned, import the LGA boundary GIS data of Nigeria downloaded from both sources recommended above.\n\n\n\n3.1.1 The geoBoundaries data set\n\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\ngeoNGA <- st_read(\"data/geospatial/\",\n                  layer =\n                    \"geoBoundaries-NGA-ADM2\") %>%\n  st_transform(crs = 26392)\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\nThe NGA data set\n\n\nNGA <- st_read(\"data/geospatial/\",\n               layer = \"nga_admbnda_adm2_osgof_20190417\") %>%\n  st_transform(crs = 26392)\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\nShow the code\nwp_nga <- read_csv(\"data/aspatial/WPdx.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\nRows: 406566 Columns: 70\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (43): #source, #report_date, #status_id, #water_source_clean, #water_sou...\ndbl (23): row_id, #lat_deg, #lon_deg, #install_year, #fecal_coliform_value, ...\nlgl  (4): #rehab_year, #rehabilitator, is_urban, latest_record\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "In-class_Ex/In-class_ex02/In-class_Ex02.html#importing",
    "href": "In-class_Ex/In-class_ex02/In-class_Ex02.html#importing",
    "title": "In-class_Ex02",
    "section": "3.2 Importing",
    "text": "3.2 Importing\n\nConverting water point data into sf point features\nConverting an aspatial data into an sf data.frame\n\n\n\nShow the code\nwp_nga$Geometry = st_as_sfc(wp_nga$'New Georeferenced Column')\nwp_nga\n\n\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\n\n\n\nShow the code\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\n\n\nTransforming into Nigeria projected coordinate system\n\n\n\nShow the code\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex02/In-class_Ex02.html#geospatial-data",
    "href": "In-class_Ex/In-class_ex02/In-class_Ex02.html#geospatial-data",
    "title": "In-class_Ex02",
    "section": "4 Geospatial Data",
    "text": "4 Geospatial Data\n\nExcluding redundant fields\n\n\n\nShow the code\nNGA <- NGA %>% \n  select (c(3:4, 8:9))\n\n\n\n\n\nChecking for duplicate name\nIt is always important to check for duplicate name in the data in the data main data fields. Using duplicated() of Base R, we can flag out LGA names that might be duplicated as shown in the code chunk below.\n\n\nNGA$ADN2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\nNULL\n\n\n\nLet us correct these errors by using the code chunk below.\n\n\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\n\nLet us check again if there is any duplicates. ::: {style=“font-size: 1.2em”}\n\nNGA$ADN2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\nNULL\n\n\n:::"
  },
  {
    "objectID": "In-class_Ex/In-class_ex02/In-class_Ex02.html#data-wrangling-for-water-point-data",
    "href": "In-class_Ex/In-class_ex02/In-class_Ex02.html#data-wrangling-for-water-point-data",
    "title": "In-class_Ex02",
    "section": "5 Data Wrangling for Water Point Data",
    "text": "5 Data Wrangling for Water Point Data\n\n\nfreq(data = wp_sf,\n     input = '#status_clean')\n\nWarning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the funModeling package.\n  Please report the issue at <https://github.com/pablo14/funModeling/issues>.\n\n\n\n\n\n                     #status_clean frequency percentage cumulative_perc\n1                       Functional     45883      48.29           48.29\n2                   Non-Functional     29385      30.93           79.22\n3                             <NA>     10656      11.22           90.44\n4      Functional but needs repair      4579       4.82           95.26\n5 Non-Functional due to dry season      2403       2.53           97.79\n6        Functional but not in use      1686       1.77           99.56\n7         Abandoned/Decommissioned       234       0.25           99.81\n8                        Abandoned       175       0.18           99.99\n9 Non functional due to dry season         7       0.01          100.00\n\n\n\nRename NA to unknown. ::: {style=“font-size: 1.2em”}\n\nwp_sf_nga <- wp_sf %>%\n  rename(status_clean = '#status_clean') %>%\n  select(status_clean) %>%\n  mutate(status_clean = replace_na(\n    status_clean, \"unknown\"))\n\n:::\n\nExtracting Water Point Data\nNow we are ready to extract the water point daa according to their status.\nThe code chunk below is used to extract functional water point.\n\n\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\n\nThe code chunk below is used to extract nonfunctional water point.\n\n\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Abandoned/Decommissioned\",\n             \"Abandoned\",\n             \"Non-Functional due to dry season\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\"))\n\n\nThe code chunk below is used to extract water point ::: {style=“font-size: 1.2em”}\n\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\n:::\n\n\nPerforming Point-in-Polygon Count\n\n\nNGA_wp <- NGA %>%\n  mutate (`total_wp` = lengths(\n    st_intersects(NGA, wp_sf_nga))) %>%\n  mutate (`wp_functional` = lengths(\n    st_intersects(NGA, wp_functional))) %>%\n  mutate (`wp_nonfunctional` = lengths(\n    st_intersects(NGA, wp_nonfunctional))) %>%\n  mutate (`wp_unknown` = lengths(\n    st_intersects(NGA, wp_unknown))) \n\n\n\n\nSaving the analytical data in rds format\nIn order to retain the sf object structure for subsequent analysis, it is recommended to save the sf data.frame into rds format.\nIn the code chunk below, write_rds() of readr package is used to export an sf data.frame into rds format.\n\n\nwrite_rds(NGA_wp, \"data/rds/NGA_WP.rds\")\n\n\n\nVisualising attributes by using statistical graphs\n\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) +\n  geom_histogram(bins=20, color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T  )),\n              color = \"red\",\n              linetype=\"dashed\",\n              size=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle=0))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "In-class_Ex/In-class_ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_ex03/In-class_Ex03.html",
    "title": "in-class_ex03: Analytical Mapping",
    "section": "",
    "text": "pacman::p_load(tmap, tidyverse, sf)\n\n\n\n\n\nNGA_wp <- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_ex03/In-class_Ex03.html#choropleth-map-for-rates",
    "href": "In-class_Ex/In-class_ex03/In-class_Ex03.html#choropleth-map-for-rates",
    "title": "in-class_ex03: Analytical Mapping",
    "section": "Choropleth Map for Rates",
    "text": "Choropleth Map for Rates\n\nDeriving Proportion of Functional Water Points and Non-Functional Water Points\n\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\nPlotting map of rate\nPlot a choropleth map showing the distribution of percentage functional water point by LGA\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n=10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_ex04/In-class_Ex04.html",
    "title": "In-class_Ex04",
    "section": "",
    "text": "pacman::p_load(maptools, sf, raster, spatstat, tmap)\n\n\n\n\n\nchildcare_sf <-\n  st_read(\"data/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex04\\data\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf <- st_read(dsn = \"data\", \n                 layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex04\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf <- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex04\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots(alph=1,\n          size=0.01)+\n  tm_view (set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\n\n\n\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\n\n\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\n\n\n\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 1545 points\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\nplot(childcare_ppp)\n\n\n\n\n\nsummary(childcare_ppp)\n\nPlanar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units"
  },
  {
    "objectID": "In-class_Ex/In-class_ex05/data/stores.html",
    "href": "In-class_Ex/In-class_ex05/data/stores.html",
    "title": "is415-gaa-michellefaith",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>"
  },
  {
    "objectID": "In-class_Ex/In-class_ex05/data/study_area.html",
    "href": "In-class_Ex/In-class_ex05/data/study_area.html",
    "title": "is415-gaa-michellefaith",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>"
  },
  {
    "objectID": "In-class_Ex/In-class_ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_ex05/In-class_Ex05.html",
    "title": "In-class_Ex05",
    "section": "",
    "text": "pacman::p_load(tidyverse, tmap, sf, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex05/In-class_Ex05.html#importing-data",
    "href": "In-class_Ex/In-class_ex05/In-class_Ex05.html#importing-data",
    "title": "In-class_Ex05",
    "section": "Importing Data",
    "text": "Importing Data\n\nstudyArea <- st_read(dsn = \"data\",\n                     layer = \"study_area\") %>%\n  st_transform(crs=3829)\n\nReading layer `study_area' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex05\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 121.4836 ymin: 25.00776 xmax: 121.592 ymax: 25.09288\nGeodetic CRS:  TWD97\n\n\n\nstores <- st_read(dsn = \"data\",\n                     layer = \"stores\") %>%\n  st_transform(crs=3829)\n\nReading layer `stores' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex05\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1409 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 121.4902 ymin: 25.01257 xmax: 121.5874 ymax: 25.08557\nGeodetic CRS:  TWD97\n\n\n\nVisualising the sf layers\n\ntmap_mode(\"view\")\ntm_shape(studyArea) +\n  tm_polygons() +\n  tm_shape(stores) +\n  tm_dots(col = \"Name\",\n          size = 0.01,\n          border.col = \"black\",\n          border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(12,16))\n\n\n\n\n\n\n\n\nLocal Colocation Quotients (LClQ)\nsearch for the 6 nearest neighbour.\n\nnb <- include_self(\n  st_knn(st_geometry(stores, 6))\n)\n\nwt <- st_kernel_weights(nb,\n                        stores,\n                        \"gaussian\",\n                        adaptive = TRUE)\n\nFamilyMart <- stores %>%\n  filter(Name == \"Family Mart\")\nA <- FamilyMart$Name\n\nSevenEleven <- stores %>%\n  filter(Name == \"7-Eleven\")\nB <- SevenEleven$Name\n\nLCLQ <- local_colocation(A, B, nb, wt, 49)\n\nLCLQ_stores <- cbind(stores, LCLQ)\n\ntmap_mode(\"view\")\ntm_shape(studyArea) +\n  tm_polygons() +\n  tm_shape(LCLQ_stores) +\n  tm_dots(col = \"X7.Eleven\",\n          size = 0.01,\n          border.col = \"black\",\n          border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(12,16))"
  },
  {
    "objectID": "In-class_Ex/In-class_ex06/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_ex06/In-class_Ex06.html",
    "title": "In-class Exercise 6: Spatial Weights - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Hands-on Exercise 6. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_ex06/In-class_Ex06.html#getting-started",
    "href": "In-class_Ex/In-class_ex06/In-class_Ex06.html#getting-started",
    "title": "In-class Exercise 6: Spatial Weights - sfdep methods",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\npacman::p_load(sf, sfdep, tmap, tidyverse)\n\n\n\nThe Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nImporting geospatial data\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImporting attribute table\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\nCombining both data frame by using left join\n\nhunan_GDPPC <- left_join(hunan, hunan2012) %>%\n  select(1:4, 7, 15)\n\n\n\nPlotting a choropleth map\n\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by district, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\nDeriving Contiguity Spatial Weights\nBy and large, there are two types of spatial weights, they are contiguity wights and distance-based weights. In this section, you will learn how to derive contiguity spatial weights by using sfdep.\nTwo steps are required to derive a contiguity spatial weights, they are:\n\nidentifying contiguity neighbour list by st_contiguity() of sfdep package, and\nderiving the contiguity spatial weights by using st_weights() of sfdep package\n\n\nIdentifying contiguity neighbours: Queen’s method\nIn the code chunk below st_contiguity() is used to derive a contiguity neighbour list by using Queen’s method.\n\nnb_queen <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\n\nBy default, queen argument is TRUE. If you do not specify queen = FALSE, this function will return a list of first order neighbours by using the Queen criteria. Rooks method will be used to identify the first order neighbour if queen = FALSE is used.\nThe code chunk below is used to print the summary of the first lag neighbour list (i.e. nb) .\n\nsummary(nb_queen$nb)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan province. The most connected area unit has 11 neighbours. There are two are units with only one neighbour.\nTo view the content of the data table, you can either display the output data frame on RStudio data viewer or by printing out the first ten records by using the code chunk below.\n\nnb_queen\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb   NAME_2  ID_3    NAME_3   ENGTYPE_3\n1                 2, 3, 4, 57, 85  Changde 21098   Anxiang      County\n2               1, 57, 58, 78, 85  Changde 21100   Hanshou      County\n3                     1, 4, 5, 85  Changde 21101    Jinshi County City\n4                      1, 3, 5, 6  Changde 21102        Li      County\n5                     3, 4, 6, 85  Changde 21103     Linli      County\n6                4, 5, 69, 75, 85  Changde 21104    Shimen      County\n7                  67, 71, 74, 84 Changsha 21109   Liuyang County City\n8       9, 46, 47, 56, 78, 80, 86 Changsha 21110 Ningxiang      County\n9           8, 66, 68, 78, 84, 86 Changsha 21111 Wangcheng      County\n10 16, 17, 19, 20, 22, 70, 72, 73 Chenzhou 21112     Anren      County\n      County GDPPC                       geometry\n1    Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren 12761 POLYGON ((113.1757 26.82734...\n\n\nThe print shows that polygon 1 has five neighbours. They are polygons number 2, 3, 4, 57,and 85.\nYou can reveal the county name of the five neighbouring polygons of popygon No. 1 (i.e. Anxiang) by using the code chunk below.\n\nnb_queen$County[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\n\n\nIdentify contiguity neighbours: Rooks’ method\n\nnb_rook <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         .before = 1)\n\n\n\nIdentifying higher order neighbors\nThere are times that we need to identify high order contiguity neighbours. To accomplish the task, st_nb_lag_cumul() should be used as shown in the code chunk below.\n\nnb2_queen <-  hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         nb2 = st_nb_lag_cumul(nb, 2),\n         .before = 1)\n\nNote that if the order is 2, the result contains both 1st and 2nd order neighbors as shown on the print below.\n\nnb2_queen\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                                        nb2\n1                                     2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\n2                           1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\n3                                                 1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\n4                                                             1, 2, 3, 5, 6, 57, 69, 75, 85\n5                                                 1, 2, 3, 4, 6, 32, 56, 57, 69, 75, 78, 85\n6                                         1, 2, 3, 4, 5, 32, 53, 55, 56, 57, 69, 75, 78, 85\n7                                                     9, 19, 66, 67, 71, 73, 74, 76, 84, 86\n8  2, 9, 19, 21, 31, 32, 34, 35, 36, 41, 45, 46, 47, 56, 58, 66, 68, 74, 78, 80, 84, 85, 86\n9               2, 7, 8, 19, 21, 35, 46, 47, 56, 58, 66, 67, 68, 74, 76, 78, 80, 84, 85, 86\n10               11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 70, 71, 72, 73, 74, 82, 83, 86\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\nDeriving contiguity weights: Queen’s method\nNow, you are ready to compute the contiguity weights by using st_weights() of sfdep package.\n\nDeriving contiguity weights: Queen’s method\nIn the code chunk below, queen method is used to derive the contiguity weights.\n\nwm_q <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\nDeriving contiguity weights: Rooks method\n\nwm_r <- hunan %>%\n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         wt = st_weights(nb),\n         .before = 1) \n\n\n\n\nDistance-based Weights\nThere are three popularly used distance-based spatial weights, they are:\n\nfixed distance weights,\nadaptive distance weights, and\ninverse distance weights (IDW).\n\n\nDeriving fixed distance weights\nBefore we can derive the fixed distance weights, we need to determine the upper limit for distance band by using the steps below:\n\ngeo <- sf::st_geometry(hunan_GDPPC)\nnb <- st_knn(geo, longlat = TRUE)\ndists <- unlist(st_nb_dists(geo, nb))\n\nNow, we will go ahead to derive summary statistics of the nearest neighbour distances vector (i.e. dists) by using the code chunk below.\n\nsummary(dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.56   29.11   36.89   37.34   43.21   65.80 \n\n\nThe summary statistics report above shows that the maximum nearest neighbour distance is 65.80km. By using a threshold value of 66km will ensure that each area will have at least one neighbour.\nNow we will go ahead to compute the fixed distance weights by using the code chunk below.\n\nwm_fd <- hunan_GDPPC %>%\n  mutate(nb = st_dist_band(geometry,\n                           upper = 66),\n               wt = st_weights(nb),\n               .before = 1)\n\n\n\nDeriving adaptive distance weights\nIn this section, you will derive an adaptive spatial weights by using the code chunk below.\n\nwm_ad <- hunan_GDPPC %>% \n  mutate(nb = st_knn(geometry,\n                     k=8),\n         wt = st_weights(nb),\n               .before = 1)\n\n\n\nCalculate inverse distance weights\nIn this section, you will derive an inverse distance weights by using the code chunk below.\n\nwm_idw <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07a.html",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07a.html",
    "title": "In-class_Ex07: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "",
    "text": "pacman::p_load(tidyverse, tmap, sf, sfdep, plotly)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#importing-data",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#importing-data",
    "title": "In-class_Ex07: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Importing Data",
    "text": "Importing Data\n\nhunan <- st_read(dsn = \"data/geospatial\",\n                     layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#joining-using-left-join",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#joining-using-left-join",
    "title": "In-class_Ex07: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Joining using left join",
    "text": "Joining using left join\n\nhunan_GDPPC <- left_join(hunan, hunan2012) %>%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#deriving-contiguity-weights-queens-method",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#deriving-contiguity-weights-queens-method",
    "title": "In-class_Ex07: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Deriving contiguity weights: queen’s method",
    "text": "Deriving contiguity weights: queen’s method\n\nwm_q <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\n\ncomputing global moran’ I\n\nmoranI <- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n\nperforming gloabl moran’I test\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\nset.seed(1234)\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#computing-local-morans-i",
    "title": "In-class_Ex07: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\n\nlisa <- wm_q %>%\n  mutate(local_moran= local_moran(\n    GDPPC, nb, wt, nsim = 99), \n      .before = 1) %>%\n  unnest(local_moran)\nlisa\n\nSimple feature collection with 88 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 21\n         ii        eii   var_ii    z_ii    p_ii p_ii_…¹ p_fol…² skewn…³ kurtosis\n      <dbl>      <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n 1 -0.00147  0.00177    4.18e-4 -0.158  0.874      0.82    0.41  -0.812  0.652  \n 2  0.0259   0.00641    1.05e-2  0.190  0.849      0.96    0.48  -1.09   1.89   \n 3 -0.0120  -0.0374     1.02e-1  0.0796 0.937      0.76    0.38   0.824  0.0461 \n 4  0.00102 -0.0000349  4.37e-6  0.506  0.613      0.64    0.32   1.04   1.61   \n 5  0.0148  -0.00340    1.65e-3  0.449  0.654      0.5     0.25   1.64   3.96   \n 6 -0.0388  -0.00339    5.45e-3 -0.480  0.631      0.82    0.41   0.614 -0.264  \n 7  3.37    -0.198      1.41e+0  3.00   0.00266    0.08    0.04   1.46   2.74   \n 8  1.56    -0.265      8.04e-1  2.04   0.0417     0.08    0.04   0.459 -0.519  \n 9  4.42     0.0450     1.79e+0  3.27   0.00108    0.02    0.01   0.746 -0.00582\n10 -0.399   -0.0505     8.59e-2 -1.19   0.234      0.28    0.14  -0.685  0.134  \n# … with 78 more rows, 12 more variables: mean <fct>, median <fct>,\n#   pysal <fct>, nb <nb>, wt <list>, NAME_2 <chr>, ID_3 <int>, NAME_3 <chr>,\n#   ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>, geometry <POLYGON [°]>, and\n#   abbreviated variable names ¹​p_ii_sim, ²​p_folded_sim, ³​skewness\n\n\n\nvisualising local Moran’s I\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nlisa_sig <- lisa %>%\n  filter(p_ii < 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(lisa_sig) +\n  tm_fill(\"mean\") +\n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#computing-local-morans-i-1",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#computing-local-morans-i-1",
    "title": "In-class_Ex07: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "computing local Moran’s I",
    "text": "computing local Moran’s I\n\nHCSA <- wm_q %>%\n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, nsim = 99),\n      .before = 1) %>%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 88 features and 16 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 17\n    gi_star   e_gi     var_gi  p_value p_sim p_fol…¹ skewn…² kurto…³ nb    wt   \n      <dbl>  <dbl>      <dbl>    <dbl> <dbl>   <dbl>   <dbl>   <dbl> <nb>  <lis>\n 1 -0.00567 0.0115 0.00000812  9.95e-1  0.82    0.41   1.03    1.23  <int> <dbl>\n 2 -0.235   0.0110 0.00000581  8.14e-1  1       0.5    0.912   1.05  <int> <dbl>\n 3  0.298   0.0114 0.00000776  7.65e-1  0.7     0.35   0.455  -0.732 <int> <dbl>\n 4  0.145   0.0121 0.0000111   8.84e-1  0.64    0.32   0.900   0.726 <int> <dbl>\n 5  0.356   0.0113 0.0000119   7.21e-1  0.64    0.32   1.08    1.31  <int> <dbl>\n 6 -0.480   0.0116 0.00000706  6.31e-1  0.82    0.41   0.364  -0.676 <int> <dbl>\n 7  3.66    0.0116 0.00000825  2.47e-4  0.02    0.01   0.909   0.664 <int> <dbl>\n 8  2.14    0.0116 0.00000714  3.26e-2  0.16    0.08   1.13    1.48  <int> <dbl>\n 9  4.55    0.0113 0.00000656  5.28e-6  0.02    0.01   1.36    4.14  <int> <dbl>\n10  1.61    0.0109 0.00000341  1.08e-1  0.18    0.09   0.269  -0.396 <int> <dbl>\n# … with 78 more rows, 7 more variables: NAME_2 <chr>, ID_3 <int>,\n#   NAME_3 <chr>, ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>,\n#   geometry <POLYGON [°]>, and abbreviated variable names ¹​p_folded_sim,\n#   ²​skewness, ³​kurtosis"
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "In-class_Ex07: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "Hot Spot and Cold Spot Area Analysis (HCSA)\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure."
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#computing-local-gi-statistics",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07a.html#computing-local-gi-statistics",
    "title": "In-class_Ex07: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Computing local Gi* statistics",
    "text": "Computing local Gi* statistics\n\nwm_idw <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nHCSA <- wm_idw %>% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 88 features and 16 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 17\n    gi_star   e_gi     var_gi  p_value p_sim p_fol…¹ skewn…² kurto…³ nb    wts  \n      <dbl>  <dbl>      <dbl>    <dbl> <dbl>   <dbl>   <dbl>   <dbl> <nb>  <lis>\n 1  0.243   0.0108 0.00000807  8.08e-1  0.54    0.27   1.40   2.08   <int> <dbl>\n 2 -0.434   0.0119 0.0000112   6.64e-1  0.82    0.41   0.779  0.0896 <int> <dbl>\n 3  0.400   0.0111 0.00000803  6.89e-1  0.68    0.34   0.766 -0.130  <int> <dbl>\n 4  0.156   0.0120 0.0000130   8.76e-1  0.78    0.39   1.22   2.12   <int> <dbl>\n 5  0.476   0.0111 0.00000858  6.34e-1  0.58    0.29   1.15   1.43   <int> <dbl>\n 6  0.00786 0.0103 0.00000505  9.94e-1  0.9     0.45   0.648 -0.171  <int> <dbl>\n 7  4.24    0.0108 0.00000724  2.25e-5  0.02    0.01   0.721  0.225  <int> <dbl>\n 8  2.89    0.0110 0.00000482  3.82e-3  0.04    0.02   0.743  0.864  <int> <dbl>\n 9  5.87    0.0105 0.00000450  4.43e-9  0.02    0.01   0.783  0.375  <int> <dbl>\n10  1.21    0.0114 0.00000417  2.25e-1  0.26    0.13   0.705  0.403  <int> <dbl>\n# … with 78 more rows, 7 more variables: NAME_2 <chr>, ID_3 <int>,\n#   NAME_3 <chr>, ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>,\n#   geometry <POLYGON [°]>, and abbreviated variable names ¹​p_folded_sim,\n#   ²​skewness, ³​kurtosis"
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07b.html",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07b.html",
    "title": "In-class_Ex07: Emerging Hot Spot Analysis: sfdep methods",
    "section": "",
    "text": "Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\n\nBuilding a space-time cube,\nCalculating Getis-Ord local Gi* statistic for each bin by using an FDR correction,\nEvaluating these hot and cold spot trends by using Mann-Kendall trend test,\nCategorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin."
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07b.html#getting-started",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07b.html#getting-started",
    "title": "In-class_Ex07: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and Loading the R Packages\nAs usual, p_load() of pacman package will be used to check if the necessary packages have been installed in R, if yes, load the packages on R environment.\nFive R packages are need for this in-class exercise, they are: sf, sfdep, tmap, and tidyverse.\n\npacman::p_load(sf, sfdep, tmap, plotly, tidyverse, zoo)\n\n\n\nThe Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_GDPPC, an attribute data set in csv format. Before getting started, reveal the content of Hunan_GDPPC.csv by using Notepad and MS Excel.\n\n\n\nImporting geospatial data\nIn the code chunk below, st_read() of sf package is used to import Hunan shapefile into R.\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImporting attribute table\nIn the code chunk below, read_csv() of readr is used to import Hunan_GDPPC.csv into R.\n\nGDPPC <- read_csv(\"data/aspatial/Hunan_GDPPC.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_ex07/In-class_Ex07b.html#creating-a-time-series-cube",
    "href": "In-class_Ex/In-class_ex07/In-class_Ex07b.html#creating-a-time-series-cube",
    "title": "In-class_Ex07: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Creating a Time Series Cube",
    "text": "Creating a Time Series Cube\nIn the code chunk below, spacetime() of sfdep ised used to create an spatio-temporal cube.\n\nGDPPC_st <- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\nNext, is_spacetime_cube() of sfdep package will be used to verify if GDPPC_st is indeed an space-time cube object.\n\nis_spacetime_cube(GDPPC_st)\n\n[1] TRUE"
  },
  {
    "objectID": "In-class_Ex/In-class_ex08/In-class_Ex08.html",
    "href": "In-class_Ex/In-class_ex08/In-class_Ex08.html",
    "title": "In-class_Ex08: Building hedonic pricing model with gwr",
    "section": "",
    "text": "pacman::p_load(olsrr, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex08/In-class_Ex08.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_ex08/In-class_Ex08.html#geospatial-data-wrangling",
    "title": "In-class_Ex08: Building hedonic pricing model with gwr",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\n\nImporting geospatial data\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\In-class_Ex\\In-class_ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\nst_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "In-class_Ex/In-class_ex08/In-class_Ex08.html#aspatial-data-wrangling",
    "href": "In-class_Ex/In-class_ex08/In-class_Ex08.html#aspatial-data-wrangling",
    "title": "In-class_Ex08: Building hedonic pricing model with gwr",
    "section": "Aspatial Data Wrangling",
    "text": "Aspatial Data Wrangling\n\ncondo_resale = read.csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\n\n\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)\n\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n  POSTCODE SELLING_PRICE AREA_SQM AGE  PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n1   118635       3000000      309  30  7.941259      0.1659793        2.5198118\n2   288420       3880000      290  32  6.609797      0.2802725        1.9333338\n3   267833       3325000      248  33  6.898000      0.4292267        0.5021395\n4   258380       4250000      127   7  4.038861      0.3947354        1.9910316\n5   467169       1400000      145  28 11.783402      0.1194666        1.1212570\n6   466472       1320000      139  22 10.263545      0.1253106        0.7892980\n  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN  PROX_MRT PROX_PARK\n1             6.618741          1.7654221        0.05835552 0.5607188 1.1710446\n2             7.505109          0.5450761        0.61592412 0.6584461 0.1992269\n3             6.463887          0.3778930        0.14120309 0.3053433 0.2779886\n4             4.906512          1.6825997        0.38200076 0.6910183 0.9832843\n5             6.410632          0.5647840        0.46097386 0.5289100 0.1164202\n6             5.092160          0.7812047        0.09939285 0.3196637 0.4102575\n  PROX_PRIMARY_SCH PROX_TOP_PRIMARY_SCH PROX_SHOPPING_MALL PROX_SUPERMARKET\n1        1.6340256            3.3273195          2.2102717        0.9103958\n2        0.9747834            0.9747834          2.9374279        0.5900617\n3        1.4715016            1.4715016          1.2256850        0.4135583\n4        1.4546324            2.3006394          0.3525671        0.4162219\n5        0.7094603            0.7094603          1.3077767        0.5810551\n6        0.8495039            0.8495039          1.2078267        1.1630128\n  PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD LEASEHOLD_99YR\n1    0.10336166          18               0        1              0\n2    0.28673408          20               0        1              0\n3    0.28504777          27               0        1              0\n4    0.29872340          30               0        1              0\n5    0.34492758          30               0        1              0\n6    0.07236363          31               1        1              0\n                   geometry\n1 POINT (22085.12 29951.54)\n2  POINT (25656.84 34546.2)\n3  POINT (23963.99 32890.8)\n4 POINT (27044.28 32319.77)\n5 POINT (41042.56 33743.64)\n6  POINT (39717.04 32943.1)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex08/In-class_Ex08.html#eda",
    "href": "In-class_Ex/In-class_ex08/In-class_Ex08.html#eda",
    "title": "In-class_Ex08: Building hedonic pricing model with gwr",
    "section": "EDA",
    "text": "EDA\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n\nlibrary(dplyr)\ncondo_resale.sf <- condo_resale.sf %>%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n\ntmap_mode(\"view\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "is415-gaa-michellefaith",
    "section": "",
    "text": "This is for IS415 GAA AY2022-23 Semester 2."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01.html",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of functional and non-function water points in Osub State, Nigeria",
    "section": "",
    "text": "Water is an important resource to mankind. Clean and accessible water is critical to human health. It provides a healthy environment, a sustainable economy, reduces poverty and ensures peace and security. Yet over 40% of the global population does not have access to sufficient clean water. By 2025, 1.8 billion people will be living in countries or regions with absolute water scarcity, according to UN-Water. The lack of water poses a major threat to several sectors, including food security. Agriculture uses about 70% of the world’s accessible freshwater.\nDeveloping countries are most affected by water shortages and poor water quality. Up to 80% of illnesses in the developing world are linked to inadequate water and sanitation. Despite technological advancement, providing clean water to the rural community is still a major development issues in many countries globally, especially countries in the Africa continent.\nTo address the issue of providing clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library. What is so special of this project is that data are collected based on WPDx Data Standard.\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate spatial point patterns analysis methods to discover the geographical distribution of functional and non-function water points and their co-locations if any in Osun State, Nigeria.\n\n\n\n\n\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\n\n\n\nNigeria Level-2 Administrative Boundary (also known as Local Government Area) polygon features GIS data will be used in this take-home exercise. The data can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries.\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\n\n\nDerive kernel density maps of functional and non-functional water points. Using appropriate tmap functions,\nDisplay the kernel density maps on openstreetmap of Osub State, Nigeria. Describe the spatial patterns revealed by the kernel density maps.\nHighlight the advantage of kernel density map over point map.\n\n\n\n\nWith reference to the spatial point patterns observed in ESDA:\n\nFormulate the null hypothesis and alternative hypothesis and select the confidence level.\nPerform the test by using appropriate Second order spatial point patterns analysis technique.\nWith reference to the analysis results, draw statistical conclusions.\n\n\n\n\nIn this section, you are required to confirm statistically if the spatial distribution of functional and non-functional water points are independent from each other.\n\nFormulate the null hypothesis and alternative hypothesis and select the confidence level.\nPerform the test by using appropriate Second order spatial point patterns analysis technique.\nWith reference to the analysis results, draw statistical conclusions."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex01.html#getting-started",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of functional and non-function water points in Osub State, Nigeria",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nThe R packages we’ll use for this analysis are:\n\nsf: used for importing, managing, and processing geospatial data\ntidyverse: a collection of packages for data science tasks\ntmap: used for creating thematic maps, such as choropleth and bubble maps\nspatstat: used for point pattern analysis\nraster: reads, writes, manipulates, analyses and models gridded spatial data (e.g. raster-based geographical data)\nfunModeling: covers common aspects in predictive modeling (e.g. data cleaning, variable importance analysis and assessing model performance)\nsfdep: performing geospatial data wrangling and local colocation quotient analysis\nmaptools: maptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\n\n\n#pacman::p_load(tmap, tidyverse, sf, funModeling, sfdep, raster)\n\npacman::p_load(tmap, tidyverse, sf, funModeling, sfdep)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#handling-geospatial-data",
    "href": "Take-home_Ex/Take-home_Ex01.html#handling-geospatial-data",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of functional and non-function water points in Osub State, Nigeria",
    "section": "3 Handling Geospatial Data",
    "text": "3 Handling Geospatial Data\n\n3.1 Importing Geospatial Data\nWe will be importing the following geospatial datasets in R by using st_read() of sf package:\n\nThe geoBoundaries Dataset\nThe NGA data set\n\n\n3.1.1 The geoBoundaries Dataset\n\ngeoNGA <- st_read(\"data/geospatial/\",\n                  layer = \"geoBoundaries-NGA-ADM2\") %>%\n  st_transform(crs = 26392)\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\Take-home_Ex\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\n\n3.1.2 The NGA Dataset\n\nNGA <- st_read(\"data/geospatial/\",\n               layer = \"geoBoundaries-NGA-ADM2\") %>%\n  st_transform(crs = 26392)\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\Take-home_Ex\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nBy examining both sf dataframe closely, we notice that NGA provide both LGA and state information. Hence, NGA data.frame will be used for the subsequent processing.\n\n\n\n3.2 Importing Aspatial Data\nWe will use read_csv() of readr package to import only water points within Nigeria.\n\nwp_nga <- read_csv(\"data/aspatial/WPdx.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\n\n3.2.1 Converting water point data into sf point features\nConverting an aspatial data into an sf data.frame involves two steps.\nFirst, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`New Georeferenced Column`)\nwp_nga\n\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\n\n3.2.2 Transforming the Nigeria projected coordinate system\nWe will now transform the projection from wgs84 to an appropriate projected coordinate system of Nigeria.\n\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#data-wrangling-for-water-point-data",
    "href": "Take-home_Ex/Take-home_Ex01.html#data-wrangling-for-water-point-data",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of functional and non-function water points in Osub State, Nigeria",
    "section": "4 Data Wrangling for Water Point Data",
    "text": "4 Data Wrangling for Water Point Data\nExploratory Data Analysis (EDA) is a popular approach to gain initial understanding of the data. In the code chunk below, freq() of funModeling package is used to reveal the distribution of water point status visually.\n\nfreq(data = wp_sf,\n     input = '#status_clean')\n\n\n\n\n                     #status_clean frequency percentage cumulative_perc\n1                       Functional     45883      48.29           48.29\n2                   Non-Functional     29385      30.93           79.22\n3                             <NA>     10656      11.22           90.44\n4      Functional but needs repair      4579       4.82           95.26\n5 Non-Functional due to dry season      2403       2.53           97.79\n6        Functional but not in use      1686       1.77           99.56\n7         Abandoned/Decommissioned       234       0.25           99.81\n8                        Abandoned       175       0.18           99.99\n9 Non functional due to dry season         7       0.01          100.00\n\n\nFigure above shows that there are nine classes in the #status_clean fields.\nNext, code chunk below will be used to perform the following data wrangling tasksP - rename() of dplyr package is used to rename the column from #status_clean to status_clean for easier handling in subsequent steps. - select() of dplyr is used to include status_clean in the output sf data.frame. - mutate() and replace_na() are used to recode all the NA values in status_clean into unknown.\n\nwp_sf_nga <- wp_sf %>% \n  rename(status_clean = '#status_clean') %>%\n  select(status_clean) %>%\n  mutate(status_clean = replace_na(\n    status_clean, \"unknown\"))\n\n\n4.1 Extracting Water Point Data\nNow we are ready to extract the water point data according to their status.\nThe code chunk below is used to extract functional water point.\n\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nThe code chunk below is used to extract nonfunctional water point.\n\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Abandoned/Decommissioned\",\n             \"Abandoned\",\n             \"Non-Functional due to dry season\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\"))\n\nThe code chunk below is used to extract water point with unknown status.\n\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\n\n\n\n                 status_clean frequency percentage cumulative_perc\n1                  Functional     45883      87.99           87.99\n2 Functional but needs repair      4579       8.78           96.77\n3   Functional but not in use      1686       3.23          100.00\n\n\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\n\n\n\n                      status_clean frequency percentage cumulative_perc\n1                   Non-Functional     29385      91.25           91.25\n2 Non-Functional due to dry season      2403       7.46           98.71\n3         Abandoned/Decommissioned       234       0.73           99.44\n4                        Abandoned       175       0.54           99.98\n5 Non functional due to dry season         7       0.02          100.00\n\n\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n\n  status_clean frequency percentage cumulative_perc\n1      unknown     10656        100             100\n\n\n\n\n4.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\nNGA_wp <- NGA %>% \n  mutate(`total_wp` = lengths(\n    st_intersects(NGA, wp_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(NGA, wp_functional))) %>%\n  mutate(`wp_nonfunctional` = lengths(\n    st_intersects(NGA, wp_nonfunctional))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(NGA, wp_unknown)))\n\nNotice that four new derived fields have been added into NGA_wp sf data.frame.\n\n\n4.3 Visualing attributes by using statistical graphs\nIn this code chunk below, appropriate functions of ggplot2 package is used to reveal the distribution of total water points by LGA in histogram.\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#exploratory-spatial-data-analysis-esda-1",
    "href": "Take-home_Ex/Take-home_Ex01.html#exploratory-spatial-data-analysis-esda-1",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of functional and non-function water points in Osub State, Nigeria",
    "section": "5 Exploratory Spatial Data Analysis (ESDA)",
    "text": "5 Exploratory Spatial Data Analysis (ESDA)\n\n5.1 Basic Choropleth Mapping\n\n5.2.1 Visualising Distribution of Functional Water Point by LGA\n\ntmap_mode(\"view\")\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\np1\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n5.2.2 Visualising Distribution of Functional Water Point by LGA\n\ntmap_mode(\"view\")\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_nonfunctional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of nonfunctional water point by LGAs\",\n            legend.outside = FALSE)\np2\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n5.2 Spatial Patterns from Kernel Density Maps\nThe advantage of kernel density map over point map: - better visualisation - summarises information of small zones compared to other zones"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#second-order-spatial-point-patterns-analysis-1",
    "href": "Take-home_Ex/Take-home_Ex01.html#second-order-spatial-point-patterns-analysis-1",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of functional and non-function water points in Osub State, Nigeria",
    "section": "6 Second-order Spatial Point Patterns Analysis",
    "text": "6 Second-order Spatial Point Patterns Analysis\nSpatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface, using appropriate functions of spatstat. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\n\n6.1 Spatial Data Wrangling\n\npacman::p_load(maptools, raster, spatstat)\n\n\n6.1.1 Mapping the Geospatial data sets\nThis step is useful for us to plot a map to show their spatial patterns.\n\ntm_shape(wp_nonfunctional)+\n  tm_dots()\n\n\n\n\n\ntmap_mode('plot')\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying excessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\n\n6.2 Geospatial Data Wrangling\n\n6.2.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\nfunctional_sp <- as_Spatial(wp_functional)\nnonfunctional_sp <- as_Spatial(wp_nonfunctional)\n\n\nfunctional_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 52148 \nextent      : 29322.63, 1218553, 33758.37, 1092629  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 1\nnames       :              status_clean \nmin values  :                Functional \nmax values  : Functional but not in use \n\n\n\nnonfunctional_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 32204 \nextent      : 28907.91, 1209690, 33736.93, 1092883  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 1\nnames       :                     status_clean \nmin values  :                        Abandoned \nmax values  : Non functional due to dry season \n\n\nNotice that the geospatial data have been converted into their respective sp’s Spatial* classes now.\n\n\n6.2.2 Converting the generic sp format into spatstat’s ppp format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nfunctional_ppp <-as(functional_sp, \"ppp\")\nfunctional_ppp\n\nMarked planar point pattern: 52148 points\nmarks are of storage type  'character'\nwindow: rectangle = [29322.6, 1218553.3] x [33758.4, 1092628.9] units\n\n\n\nnonfunctional_ppp <-as(nonfunctional_sp, \"ppp\")\nnonfunctional_ppp\n\nMarked planar point pattern: 32204 points\nmarks are of storage type  'character'\nwindow: rectangle = [28907.9, 1209690] x [33736.9, 1092882.6] units\n\n\nNow, let us plot functional_ppp and examine the difference.\n\nplot(functional_ppp)\n\n\n\n\nYou can take a quick look at the summary statistics of the newly created ppp object by using the code chunk below.\n\nsummary(functional_ppp)\n\nMarked planar point pattern:  52148 points\nAverage intensity 4.141224e-08 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n    52148 character character \n\nWindow: rectangle = [29322.6, 1218553.3] x [33758.4, 1092628.9] units\n                    (1189000 x 1059000 units)\nWindow area = 1.25924e+12 square units\n\n\n\n\n\n6.3 Analysing Spatial Point Process Using L-Function\nIn this section, you will learn how to compute L-function estimation by using Lest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n6.3.1 Functional Water Point\n\n6.3.1.1 Computing L-Function Estimation\n\nL_f = Lest(functional_ppp, correction = \"Ripley\")\nplot(L_f, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\nThe plot above reveals that there is a sign that the distribution of Functional Water Point are not randomly distributed. However, a hypothesis test is required to confirm the observation statistically.\n\n\n6.3.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of Functional Water Point are randomly distributed.\nH1= The distribution of Functional Water Point are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with L-function.\n\nL_f.csr = envelope(functional_ppp, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,  39.\n\nDone.\n\n\n\nplot(L_f.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\nThe plot above reveals that the are signs that the distribution of Functional Water Point are not randomly distributed. Unfortunately, we failed to reject the null hypothesis because the empirical k-cross line is within the envelop of the 95% confident interval.\n\n\n\n6.3.2 Non-Functional Water Point\n\n6.3.2.1 Computing L-Function Estimation\n\nL_nf = Lest(nonfunctional_ppp, correction = \"Ripley\")\nplot(L_nf, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\nThe plot above reveals that there is a sign that the distribution of Non-Functional Water Point are not randomly distributed. However, a hypothesis test is required to confirm the observation statistically.\n\n\n6.3.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of Non-Functional Water Point are randomly distributed.\nH1= The distribution of Non-Functional Water Point are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with L-function.\n\nL_nf.csr = envelope(nonfunctional_ppp, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,  39.\n\nDone.\n\n\n\nplot(L_nf.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\nThe plot above reveals that the are signs that the distribution of Non-Functional Water Point are not randomly distributed. Unfortunately, we failed to reject the null hypothesis because the empirical k-cross line is within the envelop of the 95% confident interval."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#spatial-correlation-analysis-1",
    "href": "Take-home_Ex/Take-home_Ex01.html#spatial-correlation-analysis-1",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of functional and non-function water points in Osub State, Nigeria",
    "section": "7 Spatial Correlation Analysis",
    "text": "7 Spatial Correlation Analysis\nIn this section, we will confirm statistically if the spatial distribution of functional and non-functional water points are independent from each other.\n\n7.1 Local Colocation Quotient Analysis (LCLQ)\n\n7.1.1 Visualising the sf layers\nUsing the appropriate functions of tmap, we will be able to view the functional and non-functional water points on a single map.\n\ntmap_mode(\"view\")\ntm_shape(NGA_wp) +\n  tm_polygons() +\ntm_shape(wp_sf_nga)+ \n  tm_dots(col = \"status_clean\",\n             size = 0.01,\n             border.col = \"black\",\n             border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(5, 16))\n\n\nNotice that there are many categories for water point. For this exercise, we will have to combine all functional water point and non-functional water point into their own categories.\nFirst, we will duplicate status_clean column.\n\nwp_sf_nga$category <- wp_sf_nga$status_clean\nwp_sf_nga\n\nSimple feature collection with 95008 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 28907.91 ymin: 33736.93 xmax: 1293293 ymax: 1092883\nProjected CRS: Minna / Nigeria Mid Belt\n# A tibble: 95,008 × 3\n   status_clean            Geometry category  \n * <chr>                <POINT [m]> <chr>     \n 1 unknown      (297874.6 441473.8) unknown   \n 2 Functional   (128394.3 330487.9) Functional\n 3 unknown      (607559.4 274905.5) unknown   \n 4 unknown      (576523.1 301556.6) unknown   \n 5 unknown      (578321.7 307339.8) unknown   \n 6 unknown      (590994.2 326738.8) unknown   \n 7 unknown      (597909.2 333608.5) unknown   \n 8 unknown      (724171.9 367609.1) unknown   \n 9 unknown      (737994.1 350616.5) unknown   \n10 unknown      (749790.1 354304.6) unknown   \n# … with 94,998 more rows\n\n\nNext, we will have to change the value of “Functional but not in use” and “Functional but needs repair” into Functional.\n\nwp_sf_nga$category[wp_sf_nga$category == \"Functional but not in use\"] <- \"Functional\"\nwp_sf_nga$category[wp_sf_nga$category == \"Functional but needs repair\"] <- \"Functional\"\n\nNext, we will do the same for Non-functional.\n\nwp_sf_nga$category[wp_sf_nga$category == \"Abandoned/Decommissioned\"] <- \"Non-Functional\"\nwp_sf_nga$category[wp_sf_nga$category == \"Abandoned\"] <- \"Non-Functional\"\nwp_sf_nga$category[wp_sf_nga$category == \"Non-Functional due to dry season\"] <- \"Non-Functional\"\nwp_sf_nga$category[wp_sf_nga$category == \"Non functional due to dry season\"] <- \"Non-Functional\"\n\nWe will run tmap again to view the data.\n\ntmap_mode(\"view\")\ntm_shape(NGA_wp) +\n  tm_polygons() +\ntm_shape(wp_sf_nga)+ \n  tm_dots(col = \"category\",\n             size = 0.01,\n             border.col = \"black\",\n             border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(5, 16))\n\n\n\n\n7.1.2 Preparing nearest Neighbours List\nIn the code chunk below, st_knn() of sfdep package is used to determine the k (i.e. 6) nearest neighbours for given point geometry.\n\nnb <- include_self(\n  st_knn(st_geometry(wp_sf_nga), 6))\n\n\n\n7.1.3 Computing Kernal Weights\nIn the code chunk below, st_kernel_weights() of sfdep package is used to derive a weights list by using a kernel function.\n\nwt <- st_kernel_weights(nb, \n                        wp_sf_nga, \n                        \"gaussian\", \n                        adaptive = TRUE)\n\nFor this to work: - an object of class nb e.g. created by using either st_contiguity() or st_knn() is required. - The supported kernel methods are: “uniform”, “gaussian”, “triangular”, “epanechnikov”, or “quartic”.\n\n\n7.1.4 Preparing the Vector List\nTo compute LCLQ by using sfdep package, the reference point data must be in either character or vector list. The code chunks below are used to prepare two vector lists. One of Functional and for Non-Functional and are called A and B respectively.\n\nfunctional <- wp_sf_nga %>%\n  filter(category == \"Functional\")\nA <- functional$category\n\n\nnon_functional <- wp_sf_nga %>%\n  filter(category == \"Non-Functional\")\nB <- non_functional$category"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "",
    "text": "Since late December 2019, an outbreak of a novel coronavirus disease (COVID-19; previously known as 2019-nCoV) was reported in Wuhan, China, which had subsequently affected 210 countries worldwide. In general, COVID-19 is an acute resolved disease but it can also be deadly, with a 2% case fatality rate.\nThe COVID-19 vaccination in Indonesia is an ongoing mass immunisation in response to the COVID-19 pandemic in Indonesia. On 13 January 2021, the program commenced when President Joko Widodo was vaccinated at the presidential palace. In terms of total doses given, Indonesia ranks third in Asia and fifth in the world.\nAccording to wikipedia, as of 5 February 2023 at 18:00 WIB (UTC+7), 204,266,655 people had received the first dose of the vaccine and 175,131,893 people had been fully vaccinated; 69,597,474 of them had been inoculated with the booster or the third dose, while 1,585,164 had received the fourth dose. Jakarta has the highest percentage of population fully vaccinated with 103.46%, followed by Bali and Special Region of Yogyakarta with 85.45% and 83.02% respectively.\nDespite its compactness, the cumulative vaccination rate are not evenly distributed within DKI Jakarta. The question is where are the sub-districts with relatively higher number of vaccination rate and how they changed over time.\n\n\n\nExploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, we will apply appropriate Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatio-temporal trends of COVID-19 vaccination in DKI Jakarta.\n\n\n\n\n\nFor the purpose of this assignment, data from Riwayat File Vaksinasi DKI Jakarta will be used. Daily vaccination data are provides. You are only required to download either the first day of the month or last day of the month of the study period.\n\n\n\nFor the purpose of this study, DKI Jakarta administration boundary 2019 will be used. The data set can be downloaded at Indonesia Geospatial portal, specifically at this page.\nNote: - The national Projected Coordinates Systems of Indonesia is DGN95 / Indonesia TM-3 zone 54.1. - Exclude all the outer islands from the DKI Jakarta sf data frame, and - Retain the first nine fields in the DKI Jakarta sf data frame. The ninth field JUMLAH_PEN = Total Population.\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\n\n\nCompute the monthly vaccination rate from July 2021 to June 2022 at sub-district (also known as kelurahan in Bahasa Indonesia) level,\nPrepare the monthly vaccination rate maps by using appropriate tmap functions,\nDescribe the spatial patterns revealed by the choropleth maps (not more than 200 words).\n\n\n\n\nWith reference to the vaccination rate maps prepared in ESDA: - Compute local Gi* values of the monthly vaccination rate, - Display the Gi* maps of the monthly vaccination rate. The maps should only display the significant (i.e. p-value < 0.05). - With reference to the analysis results, draw statistical conclusions (not more than 250 words).\n\n\n\nWith reference to the local Gi* values of the vaccination rate maps prepared in the previous section: - Perform Mann-Kendall Test by using the spatio-temporal local Gi* values, - Select three sub-districts and describe the temporal trends revealed (not more than 250 words), and - Prepared a EHSA map of the Gi* values of vaccination rate. The maps should only display the significant (i.e. p-value < 0.05). - With reference to the EHSA map prepared, describe the spatial patterns revealed. (not more than 250 words)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#getting-started",
    "title": "Take-home Exercise 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "2.0 Getting Started",
    "text": "2.0 Getting Started\nThe R packages we’ll use for this analysis are:\n\nsf: used for importing, managing, and processing geospatial data\ntidyverse: a collection of packages for data science tasks\ntmap: used for creating thematic maps, such as choropleth and bubble maps\nmaptools: a set of tools for manipulating geographic data\nkableExtra: an extension of kable, used for table customisation\nplyr: used for splitting data, applying functions and combining results\nsfdep: An interface to ‘spdep’ to integrate with ‘sf’ objects and the ‘tidyverse’\nplotly: contain interactive elements that allow users to modify, explore, and experience the visualized data in new ways.\n\nIn addition, the following tidyverse packages will be used:\n\nreadxl for importing Excel worksheets (.xlsx)\ntidyr for manipulating and tidying data\ndplyr for wrangling and transforming data\nggplot2 for visualising data\n\n\n\nShow the code\n# initialise a list of required packages\n# note that 'readxl' is in the list of packages despite being part of tidyverse: as readxl is not a core tidyverse package, it needs to be loaded explicitly\npackages = c('plyr', 'sf', 'sfdep', 'tidyverse', 'readxl', 'tmap', 'maptools', 'kableExtra', 'plotly', 'zoo')\n\n# for each package, check if installed and if not, install it\nfor (p in packages){\n  if(!require(p, character.only = T)){\n    install.packages(p)\n  }\n  library(p,character.only = T)\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling-geospatial-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling-geospatial-data",
    "title": "Take-home Exercise 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "3.0 Data Wrangling: Geospatial Data",
    "text": "3.0 Data Wrangling: Geospatial Data\n\n3.1 Importing Geospatial Data\n\nbd_jakarta <- st_read(dsn=\"data/geospatial\",\n                      layer=\"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\")\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\michellefaithl\\is415-gaa-michellefaith\\Take-home_Ex\\Take-home_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84\n\n\nFrom the output message, we can see that the assigned coordinate system is WGS84. For this Indonesian-specific geospatial dataset, we would need to use the national CRS of Indonesia, DGN95, ESPG Code 23845.\n\n# transforms the CRS to DGN95, ESPG code 23845\nbd_jakarta <- st_transform(bd_jakarta, 23845)\n\nLet us check if the CRS has been properly assigned:\n\nst_crs(bd_jakarta)\n\nCoordinate Reference System:\n  User input: EPSG:23845 \n  wkt:\nPROJCRS[\"DGN95 / Indonesia TM-3 zone 54.1\",\n    BASEGEOGCRS[\"DGN95\",\n        DATUM[\"Datum Geodesi Nasional 1995\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4755]],\n    CONVERSION[\"Indonesia TM-3 zone 54.1\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",139.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9999,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",200000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",1500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre.\"],\n        AREA[\"Indonesia - onshore east of 138°E.\"],\n        BBOX[-9.19,138,-1.49,141.01]],\n    ID[\"EPSG\",23845]]\n\n\n\n\n3.2 Data Pre-Processing\nBefore we start visualising our data, we have to first check for two things: - invalid geometries, and - missing values\n\n3.2.1 Invalid Geometries\nFirstly, let us check for invalid geometries: - the st_is_valid function checks whether a geometry is valid, which returns the indices of certain values based on logical conditions.\n\n# checks for the number of geometries that are NOT valid\nlength(which(st_is_valid(bd_jakarta) == FALSE))\n\n[1] 0\n\n\nThis shows that there are no invalid geometries.\n\n\n3.2.2 Missing Values\nNext, let us check for missing values. - the rowSums(is.na(bd_jakarta))!=0 checks every row if there are NA values, returning TRUE or FALSE. - the bd_jakarta ‘wrapper’ prints said rows that contain NA values\n\nbd_jakarta[rowSums(is.na(bd_jakarta))!=0,]\n\nSimple feature collection with 2 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -3623599 ymin: 691982.5 xmax: -3620985 ymax: 693163.1\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\n    OBJECT_ID KODE_DESA             DESA   KODE    PROVINSI KAB_KOTA KECAMATAN\n243     25645  31888888     DANAU SUNTER 318888 DKI JAKARTA     <NA>      <NA>\n244     25646  31888888 DANAU SUNTER DLL 318888 DKI JAKARTA     <NA>      <NA>\n    DESA_KELUR JUMLAH_PEN JUMLAH_KK LUAS_WILAY KEPADATAN PERPINDAHA JUMLAH_MEN\n243       <NA>          0         0          0         0          0          0\n244       <NA>          0         0          0         0          0          0\n    PERUBAHAN WAJIB_KTP SILAM KRISTEN KHATOLIK HINDU BUDHA KONGHUCU KEPERCAYAA\n243         0         0     0       0        0     0     0        0          0\n244         0         0     0       0        0     0     0        0          0\n    PRIA WANITA BELUM_KAWI KAWIN CERAI_HIDU CERAI_MATI U0 U5 U10 U15 U20 U25\n243    0      0          0     0          0          0  0  0   0   0   0   0\n244    0      0          0     0          0          0  0  0   0   0   0   0\n    U30 U35 U40 U45 U50 U55 U60 U65 U70 U75 TIDAK_BELU BELUM_TAMA TAMAT_SD SLTP\n243   0   0   0   0   0   0   0   0   0   0          0          0        0    0\n244   0   0   0   0   0   0   0   0   0   0          0          0        0    0\n    SLTA DIPLOMA_I DIPLOMA_II DIPLOMA_IV STRATA_II STRATA_III BELUM_TIDA\n243    0         0          0          0         0          0          0\n244    0         0          0          0         0          0          0\n    APARATUR_P TENAGA_PEN WIRASWASTA PERTANIAN NELAYAN AGAMA_DAN PELAJAR_MA\n243          0          0          0         0       0         0          0\n244          0          0          0         0       0         0          0\n    TENAGA_KES PENSIUNAN LAINNYA GENERATED KODE_DES_1 BELUM_ MENGUR_ PELAJAR_\n243          0         0       0      <NA>       <NA>      0       0        0\n244          0         0       0      <NA>       <NA>      0       0        0\n    PENSIUNA_1 PEGAWAI_ TENTARA KEPOLISIAN PERDAG_ PETANI PETERN_ NELAYAN_1\n243          0        0       0          0       0      0       0         0\n244          0        0       0          0       0      0       0         0\n    INDUSTR_ KONSTR_ TRANSP_ KARYAW_ KARYAW1 KARYAW1_1 KARYAW1_12 BURUH BURUH_\n243        0       0       0       0       0         0          0     0      0\n244        0       0       0       0       0         0          0     0      0\n    BURUH1 BURUH1_1 PEMBANT_ TUKANG TUKANG_1 TUKANG_12 TUKANG__13 TUKANG__14\n243      0        0        0      0        0         0          0          0\n244      0        0        0      0        0         0          0          0\n    TUKANG__15 TUKANG__16 TUKANG__17 PENATA PENATA_ PENATA1_1 MEKANIK SENIMAN_\n243          0          0          0      0       0         0       0        0\n244          0          0          0      0       0         0       0        0\n    TABIB PARAJI_ PERANCA_ PENTER_ IMAM_M PENDETA PASTOR WARTAWAN USTADZ JURU_M\n243     0       0        0       0      0       0      0        0      0      0\n244     0       0        0       0      0       0      0        0      0      0\n    PROMOT ANGGOTA_ ANGGOTA1 ANGGOTA1_1 PRESIDEN WAKIL_PRES ANGGOTA1_2\n243      0        0        0          0        0          0          0\n244      0        0        0          0        0          0          0\n    ANGGOTA1_3 DUTA_B GUBERNUR WAKIL_GUBE BUPATI WAKIL_BUPA WALIKOTA WAKIL_WALI\n243          0      0        0          0      0          0        0          0\n244          0      0        0          0      0          0        0          0\n    ANGGOTA1_4 ANGGOTA1_5 DOSEN GURU PILOT PENGACARA_ NOTARIS ARSITEK AKUNTA_\n243          0          0     0    0     0          0       0       0       0\n244          0          0     0    0     0          0       0       0       0\n    KONSUL_ DOKTER BIDAN PERAWAT APOTEK_ PSIKIATER PENYIA_ PENYIA1 PELAUT\n243       0      0     0       0       0         0       0       0      0\n244       0      0     0       0       0         0       0       0      0\n    PENELITI SOPIR PIALAN PARANORMAL PEDAGA_ PERANG_ KEPALA_ BIARAW_ WIRASWAST_\n243        0     0      0          0       0       0       0       0          0\n244        0     0      0          0       0       0       0       0          0\n    LAINNYA_12 LUAS_DESA KODE_DES_3 DESA_KEL_1 KODE_12\n243          0         0       <NA>       <NA>       0\n244          0         0       <NA>       <NA>       0\n                          geometry\n243 MULTIPOLYGON (((-3620985 69...\n244 MULTIPOLYGON (((-3622382 69...\n\n\nWe can see that there are 2 NA values. We can remove rows that have an NA value by using the code below:\n\n# removes rows that have an NA value in DESA_KELUR\n# in context of this data, we can use other columns, such as KAB_KOTA or KECAMATAN\n# but since we're looking at this on a sub-district level, DESA_KELUR seemed most appropriate\nbd_jakarta <- na.omit(bd_jakarta,c(\"DESA_KELUR\"))\n\n\n\n\n3.3 Removal of Outer Islands\nWe have finished the standard pre-processing. Now, let’s visualise our data!\n\n# plots the geometry only\n# alternatively, we can use plot(bd_jakarta$geometry)\nplot(st_geometry(bd_jakarta))\n\n\n\n\nFrom our output as shown above, we can see that bd_jakarta include outer and inner islands. From our task requirement, we only need inner islands. Hence, we will need to remove them.\nFrom 3.2.2’s output, we can focus on the KAB_KOTA (City), KECAMATAN (District) and DESA_KELUR (Sub-district). After translation, we can tell that KAB_KOTA would have the coarsest-grained level of distinction. Hence, let’s check it’s unique values.\n\nunique(bd_jakarta$\"KAB_KOTA\")\n\n[1] \"JAKARTA BARAT\"    \"JAKARTA PUSAT\"    \"KEPULAUAN SERIBU\" \"JAKARTA UTARA\"   \n[5] \"JAKARTA TIMUR\"    \"JAKARTA SELATAN\" \n\n\nLooking at the output, we can see that all cities within Jakarta has the prefix of Jakarta. There is an outlier KEPULAUAN SERIBU, which means ‘Thousand Islands’, referring to the outer islands. We shall visualise this to ensure our assumption is correct.\n\n# with bd_jakarta as the input data (setting the 'base')\n# draw the KAB_KOTA (city) polygons\n# essentially shades the map according to the city divisions\ntm_shape(bd_jakarta) + \n  tm_polygons(\"KAB_KOTA\")\n\n\n\n\nSince we have identified the outer islands, let’s remove them.\n\nbd_jakarta <- filter(bd_jakarta, KAB_KOTA != \"KEPULAUAN SERIBU\")\n\n\n\n3.4 Retaining first 9 Columns\nFor this analysis, only the first 9 columns are relevant.\n\n# filters out other fields by accepting only the first 9 fields\nbd_jakarta <- bd_jakarta[, 0:9]\n\n\n\n3.5 Renaming Columns using Translation\nLet’s translate the column names of bd_jakarta so that we can better understand what it means.\n\n\nShow the code\n# with reference to: https://www.codegrepper.com/code-examples/r/rename+column+name+in+r\n# renames the columns in the style New_Name = OLD_NAME\nbd_jakarta <- bd_jakarta %>% \n  dplyr::rename(\n    Object_ID=OBJECT_ID,\n    Province=PROVINSI, \n    City=KAB_KOTA, \n    District=KECAMATAN, \n    Village_Code=KODE_DESA, \n    Village=DESA, \n    Sub_District=DESA_KELUR,\n    Code=KODE, \n    Total_Population=JUMLAH_PEN\n    )\n\n\n\n\n3.6 Initial EDA\nLet’s take a quick look at our dataset to better understand what it contains.\n\nglimpse(bd_jakarta)\n\nRows: 261\nColumns: 10\n$ Object_ID        <dbl> 25477, 25478, 25397, 25400, 25390, 25391, 25394, 2538…\n$ Village_Code     <chr> \"3173031006\", \"3173031007\", \"3171031003\", \"3171031006…\n$ Village          <chr> \"KEAGUNGAN\", \"GLODOK\", \"HARAPAN MULIA\", \"CEMPAKA BARU…\n$ Code             <dbl> 317303, 317303, 317103, 317103, 317102, 317102, 31710…\n$ Province         <chr> \"DKI JAKARTA\", \"DKI JAKARTA\", \"DKI JAKARTA\", \"DKI JAK…\n$ City             <chr> \"JAKARTA BARAT\", \"JAKARTA BARAT\", \"JAKARTA PUSAT\", \"J…\n$ District         <chr> \"TAMAN SARI\", \"TAMAN SARI\", \"KEMAYORAN\", \"KEMAYORAN\",…\n$ Sub_District     <chr> \"KEAGUNGAN\", \"GLODOK\", \"HARAPAN MULIA\", \"CEMPAKA BARU…\n$ Total_Population <dbl> 21609, 9069, 29085, 41913, 15793, 33383, 35906, 21828…\n$ geometry         <MULTIPOLYGON [m]> MULTIPOLYGON (((-3626874 69..., MULTIPOL…\n\n\n\nlength(unique(bd_jakarta$\"Sub_District\"))\n\n[1] 261\n\n\n\nlength(unique(bd_jakarta$\"District\"))\n\n[1] 42\n\n\nFrom these outputs, we can tell that there are 261 unique sub-districts and 42 unique districts. However, the max number of categories for mapping using tmap is 30. Even though we can adjust the max.categories in tmap_options, it would be hard to view and comprehend 42 and 261 categories. Hence, the only level which is comfortable to view would be the ‘City’ level.\n\n# shades the map according to the city divisions\ntm_shape(bd_jakarta) + \n  tm_polygons(\"City\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling-aspatial",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling-aspatial",
    "title": "Take-home Exercise 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "4.0 Data Wrangling: Aspatial",
    "text": "4.0 Data Wrangling: Aspatial\n\n4.1 Pre-importing EDA\nIn our ‘data/aspatial’ folder, we have multiple .xlsx files ranging from 1 July 2021 to 1 June 2022. However, before we start compiling all of our data, it’s important to understand what we’re working with and to check for any discrepancies, so let’s perform a brief EDA:\n\n# reads the 1st July 2021 .xlsx aspatial data and stores it as jul2021\njul2021 <- read_xlsx(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Juli 2021).xlsx\")\nglimpse(jul2021)\n\nRows: 268\nColumns: 21\n$ `KODE KELURAHAN`                             <chr> NA, \"3172051003\", \"317304…\n$ `WILAYAH KOTA`                               <chr> NA, \"JAKARTA UTARA\", \"JAK…\n$ KECAMATAN                                    <chr> NA, \"PADEMANGAN\", \"TAMBOR…\n$ KELURAHAN                                    <chr> \"TOTAL\", \"ANCOL\", \"ANGKE\"…\n$ SASARAN                                      <dbl> 7739060, 20393, 25785, 25…\n$ `BELUM VAKSIN`                               <dbl> 5041111, 13272, 16477, 18…\n$ `JUMLAH\\r\\nDOSIS 1`                          <dbl> 2696017, 7114, 9299, 6301…\n$ `JUMLAH\\r\\nDOSIS 2`                          <dbl> 1181740, 3287, 3221, 2644…\n$ `TOTAL VAKSIN\\r\\nDIBERIKAN`                  <dbl> 3877757, 10401, 12520, 89…\n$ `LANSIA\\r\\nDOSIS 1`                          <dbl> 502873, 1297, 2003, 820, …\n$ `LANSIA\\r\\nDOSIS 2`                          <dbl> 449073, 1128, 1746, 703, …\n$ `LANSIA TOTAL \\r\\nVAKSIN DIBERIKAN`          <dbl> 951946, 2425, 3749, 1523,…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 1`                  <dbl> 2084559, 5672, 7121, 5170…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 2`                  <dbl> 655641, 2053, 1364, 1731,…\n$ `PELAYAN PUBLIK TOTAL\\r\\nVAKSIN DIBERIKAN`   <dbl> 2740200, 7725, 8485, 6901…\n$ `GOTONG ROYONG\\r\\nDOSIS 1`                   <dbl> 35263, 44, 84, 103, 58, 7…\n$ `GOTONG ROYONG\\r\\nDOSIS 2`                   <dbl> 11624, 13, 31, 24, 18, 25…\n$ `GOTONG ROYONG TOTAL\\r\\nVAKSIN DIBERIKAN`    <dbl> 46887, 57, 115, 127, 76, …\n$ `TENAGA KESEHATAN\\r\\nDOSIS 1`                <dbl> 73322, 101, 91, 208, 70, …\n$ `TENAGA KESEHATAN\\r\\nDOSIS 2`                <dbl> 65402, 93, 80, 186, 63, 3…\n$ `TENAGA KESEHATAN TOTAL\\r\\nVAKSIN DIBERIKAN` <dbl> 138724, 194, 171, 394, 13…\n\n\n\n\n4.2 Creating an Aspatial Data Pre-processing Function\nThese are the requirements for our aspatial data:\n\nonly need particular columns of interest (listed below) -\nwe need to create an extra Date column that has the month and year of the observation, which is also in the file name\n\neach file has a regular format: Data Vaksinasi Berbasis Kelurahan(DD Month YYYY)\nthe months in the naming format are in Bahasa Indonesia - we should convert it to English for ease of comprehension\n\n\nOur columns of interest, relevant to our analysis, are as follows:\n\nKODE KELURAHAN (village code)\nWILAYAH KOTA (city)\nKECAMATAN (district)\nSASARAN (target)\nBELUM VAKSIN (yet to be vaccinated)\n\nNow that we know our requirements, we can do this step-by-step: - importing all the files into one df, - retaining the necessary columns (and deleting duplicate columns), - adding the date and year columns.\nAlternatively, we can combine all of these into a function!\n\n\nShow the code\n# takes in an aspatial data filepath and returns a processed output\naspatial_preprocess <- function(filepath){\n  # the .name_repair=\"minimal\" is to indicate not to replace duplicate column names with 'NAME...n' like we saw above!\n  # reference: https://readxl.tidyverse.org/articles/articles/column-names.html\n  result_file <- read_xlsx(filepath, .name_repair=\"minimal\")\n  \n  # Create the Date Column\n  # the format of our files is: Data Vakasinasi Berbasis Kelurhan (DD Month YYYY)\n  # while the start is technically \"(\", \"(\" is part of a regular expression and leads to a warning message, so we'll use \"Corona\" instead. The [[1]] refers to the first element in the list.\n  # we're loading it as DD-Month-YYYY format\n  # as such, the most relevant functions are substr (returns a substring) and either str_locate (returns location of substring as an integer matrix) or gregexpr (returns a list of locations of substring)\n  # reference https://stackoverflow.com/questions/14249562/find-the-location-of-a-character-in-string\n  startpoint <- gregexpr(pattern=\"Kelurahan\", filepath)[[1]] + 11\n  endpoint <- gregexpr(pattern=\")\", filepath)[[1]] - 1\n  result_file$Date <- substr(filepath, startpoint, endpoint)\n  \n  # Create the Year Column\n  startpoint <- gregexpr(pattern=\")\", filepath)[[1]] - 4\n  endpoint <- gregexpr(pattern=\")\", filepath)[[1]] - 1\n  result_file$Year <- strtoi(substr(filepath, startpoint, endpoint))\n  \n  # Retain the Relevant Columns\n  result_file <- result_file %>% \n    select(\"Date\", \n           \"Year\",\n           \"KODE KELURAHAN\", \n           \"WILAYAH KOTA\", \n           \"KECAMATAN\",\n           \"KELURAHAN\",\n           \"SASARAN\",\n           \"BELUM VAKSIN\")\n  return(result_file)\n}\n\n\n\n\n4.3 Feeding Files into our aspatial_preprocess function\nNow that we have our custom function to preprocess aspatial data, we need to feed file into it. There’s the option of doing it manually, inputting our file names line by line - but with a handy function called list.files() and lapply() (that applies a function to all elements in a list), the process can be cut down to a few lines!\n\n\nShow the code\n# in the folder 'data/aspatial', find files with the extension '.xlsx' and add it to our fileslist \n# the full.names=TRUE prepends the directory path to the file names, giving a relative file path - otherwise, only the file names (not the paths) would be returned \n# reference: https://stat.ethz.ch/R-manual/R-devel/library/base/html/list.files.html\nfileslist <-list.files(path = \"data/aspatial\", pattern = \"*.xlsx\", full.names=TRUE)\n\n# afterwards, for every element in fileslist, apply aspatial_process function\ndflist <- lapply(seq_along(fileslist), function(x) aspatial_preprocess(fileslist[x]))\n\n\nLastly, we’ll need to convert the dflist into an actual dataframe with Idply().\n\ncases_jakarta <- ldply(dflist, data.frame)\n\nLet’s check what cases_jakarta looks like, and make sure the columns are correct:\n\nglimpse(cases_jakarta)\n\nRows: 3,216\nColumns: 8\n$ Date           <chr> \"01 Agustus 2021\", \"01 Agustus 2021\", \"01 Agustus 2021\"…\n$ Year           <int> 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2…\n$ KODE.KELURAHAN <chr> NA, \"3172051003\", \"3173041007\", \"3175041005\", \"31750310…\n$ WILAYAH.KOTA   <chr> NA, \"JAKARTA UTARA\", \"JAKARTA BARAT\", \"JAKARTA TIMUR\", …\n$ KECAMATAN      <chr> NA, \"PADEMANGAN\", \"TAMBORA\", \"KRAMAT JATI\", \"JATINEGARA…\n$ KELURAHAN      <chr> \"TOTAL\", \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER…\n$ SASARAN        <dbl> 8941211, 23947, 29381, 29074, 9752, 26285, 21566, 23886…\n$ BELUM.VAKSIN   <dbl> 4399496, 12155, 13727, 18226, 4987, 13716, 10232, 9999,…\n\n\n\n\n4.4 Formatting Date Column\nSince the values in the Date column were derived from substrings, they’re naturally in string format. We should convert that into datetime, keeping in mind that the values in Date are in Bahasa Indonesia.\n\n\nShow the code\n# parses the 'Date' column into Month(Full Name)-YYYY datetime objects\n# reference: https://stackoverflow.com/questions/53380650/b-y-date-conversion-gives-na\n\n# locale=\"ind\" means that the locale has been set as Indonesia\nSys.setlocale(locale=\"ind\")\n\n\n[1] \"LC_COLLATE=Indonesian_Indonesia.1252;LC_CTYPE=Indonesian_Indonesia.1252;LC_MONETARY=Indonesian_Indonesia.1252;LC_NUMERIC=C;LC_TIME=Indonesian_Indonesia.1252\"\n\n\n\n\nShow the code\ncases_jakarta$Date <- c(cases_jakarta$Date) %>% \n  as.Date(cases_jakarta$Date, format =\"%d %B %Y\")\n\nglimpse(cases_jakarta)\n\n\nRows: 3,216\nColumns: 8\n$ Date           <date> 2021-08-01, 2021-08-01, 2021-08-01, 2021-08-01, 2021-0~\n$ Year           <int> 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2~\n$ KODE.KELURAHAN <chr> NA, \"3172051003\", \"3173041007\", \"3175041005\", \"31750310~\n$ WILAYAH.KOTA   <chr> NA, \"JAKARTA UTARA\", \"JAKARTA BARAT\", \"JAKARTA TIMUR\", ~\n$ KECAMATAN      <chr> NA, \"PADEMANGAN\", \"TAMBORA\", \"KRAMAT JATI\", \"JATINEGARA~\n$ KELURAHAN      <chr> \"TOTAL\", \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER~\n$ SASARAN        <dbl> 8941211, 23947, 29381, 29074, 9752, 26285, 21566, 23886~\n$ BELUM.VAKSIN   <dbl> 4399496, 12155, 13727, 18226, 4987, 13716, 10232, 9999,~\n\n\n\n\n4.5 Renaming Columns with Translation\nWe’ll translate the column names to English for ease of comprehension.\n\n\nShow the code\n# renames the columns in the style New_Name = OLD_NAME\ncases_jakarta <- cases_jakarta %>% \n  dplyr::rename(\n    Date=Date,\n    Village_Code=KODE.KELURAHAN,\n    Sub_District=KELURAHAN,\n    City=WILAYAH.KOTA, \n    District=KECAMATAN, \n    Target=SASARAN, \n    To_Be_Vaccinated=BELUM.VAKSIN\n    )\n\n\n\n\n4.6 Further Data Processing\nNow that our dataframe is confirmed, let’s execute any pre-processing that we might have missed.\nFirstly, let’s check for missing values.\n\n# returns rows that contain NA values\ncases_jakarta[rowSums(is.na(cases_jakarta))!=0,]\n\n           Date Year Village_Code City District Sub_District  Target\n1    2021-08-01 2021         <NA> <NA>     <NA>        TOTAL 8941211\n269  2022-04-01 2022         <NA> <NA>     <NA>        TOTAL 8941211\n537  2021-12-01 2021         <NA> <NA>     <NA>        TOTAL 8941211\n805  2022-02-01 2022         <NA> <NA>     <NA>        TOTAL 8941211\n1073 2022-01-01 2022         <NA> <NA>     <NA>        TOTAL 8941211\n1341 2021-07-01 2021         <NA> <NA>     <NA>        TOTAL 7739060\n1609 2022-06-01 2022         <NA> <NA>     <NA>        TOTAL 8941211\n1877 2022-03-01 2022         <NA> <NA>     <NA>        TOTAL 8941211\n2145 2022-05-01 2022         <NA> <NA>     <NA>        TOTAL 8941211\n2413 2021-11-01 2021         <NA> <NA>     <NA>        TOTAL 8941211\n2681 2021-10-01 2021         <NA> <NA>     <NA>        TOTAL 8941211\n2949 2021-09-01 2021         <NA> <NA>     <NA>        TOTAL 8941211\n     To_Be_Vaccinated\n1             4399496\n269           1481006\n537           1718787\n805           1536737\n1073          1620250\n1341          5041111\n1609          1444901\n1877          1516200\n2145          1455001\n2413          1875655\n2681          2221074\n2949          3259430\n\n\nSince these rows have missing values from their village_code to subdistrict, they should be removed using the code below.\n\n# removes rows that have an NA value in village_code\ncases_jakarta <- na.omit(cases_jakarta,c(\"village_code\"))\n\nNow, we’re done with the data importing and pre-processing, and we’re ready to move on to the next section!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#geospatial-data-integration",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#geospatial-data-integration",
    "title": "Take-home Exercise 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "5.0 Geospatial Data Integration",
    "text": "5.0 Geospatial Data Integration\n\n5.1 Preliminary joining + new column + EDA\nNow that we have both the geospatial and aspatial data frames, we’ll need to join them. A quick look at their headers tell us what their common fields are:\n\n\nShow the code\n# checks for column names of the dataframes\ncolnames(bd_jakarta)\n\n\n [1] \"Object_ID\"        \"Village_Code\"     \"Village\"          \"Code\"            \n [5] \"Province\"         \"City\"             \"District\"         \"Sub_District\"    \n [9] \"Total_Population\" \"geometry\"        \n\n\n\ncolnames(cases_jakarta)\n\n[1] \"Date\"             \"Year\"             \"Village_Code\"     \"City\"            \n[5] \"District\"         \"Sub_District\"     \"Target\"           \"To_Be_Vaccinated\"\n\n\nFrom this, Village_Code, District, City and “Village should match up. Let’s try doing that first.\n\n# joins cases_jakarta to bd_jakarta based on Sub_District\ncombined_jakarta <- left_join(bd_jakarta, cases_jakarta,\n                              by=c(\n                                \"Sub_District\"=\"Sub_District\")\n                              )\n\nLet’s look at the columns that we have:\n\ncolnames(combined_jakarta)\n\n [1] \"Object_ID\"        \"Village_Code.x\"   \"Village\"          \"Code\"            \n [5] \"Province\"         \"City.x\"           \"District.x\"       \"Sub_District\"    \n [9] \"Total_Population\" \"Date\"             \"Year\"             \"Village_Code.y\"  \n[13] \"City.y\"           \"District.y\"       \"Target\"           \"To_Be_Vaccinated\"\n[17] \"geometry\"        \n\n\nNow, let’s visualise our current combined_jakarta in terms of Target:\n\n\nShow the code\ntarget = tm_shape(combined_jakarta)+\n  tm_fill(\"Target\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Target\")\ntarget\n\n\n\n\n\nFrom the map, we can tell that there are still some ‘missing’ values. But this does not make sense because we have already deleted rows with NA values. This is probably due to mismatched records and we need to investigate the dataframe.\n\n\n5.2 Identifying Mismatched District Records\n\n# checks for unique values of District in cases_jakarta that aren't already present in bd_jakarta and vice versa\ncases_subdistrict <- c(cases_jakarta$Sub_District)\nbd_subdistrict <- c(bd_jakarta$Sub_District)\n\nunique(cases_subdistrict[!(cases_subdistrict %in% bd_subdistrict)])\n\n [1] \"BALE KAMBANG\"          \"HALIM PERDANA KUSUMAH\" \"JATI PULO\"            \n [4] \"KAMPUNG TENGAH\"        \"KERENDANG\"             \"KRAMAT JATI\"          \n [7] \"PAL MERIAM\"            \"PINANG RANTI\"          \"PULAU HARAPAN\"        \n[10] \"PULAU KELAPA\"          \"PULAU PANGGANG\"        \"PULAU PARI\"           \n[13] \"PULAU TIDUNG\"          \"PULAU UNTUNG JAWA\"     \"RAWA JATI\"            \n\n\n\nunique(bd_subdistrict[!(bd_subdistrict %in% cases_subdistrict)])\n\n[1] \"KRENDANG\"             \"RAWAJATI\"             \"TENGAH\"              \n[4] \"BALEKAMBANG\"          \"PINANGRANTI\"          \"JATIPULO\"            \n[7] \"PALMERIAM\"            \"KRAMATJATI\"           \"HALIM PERDANA KUSUMA\"\n\n\n\n\n5.3 Correcting Mismatched Sub-District Records\nNow that we know which sub-district records are mismatched, we need to rectify the mismatches by renaming them.\n\n\nShow the code\n# where bd_jakarta is a mismatched value, replace with the correct value\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'BALEKAMBANG'] <- 'BALE KAMBANG'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'HALIM PERDANA KUSUMA'] <- 'HALIM PERDANA KUSUMAH'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'JATIPULO'] <- 'JATI PULO'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KALI BARU'] <- 'KALIBARU'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'TENGAH'] <- 'KAMPUNG TENGAH'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KRAMATJATI'] <- 'KRAMAT JATI'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KRENDANG'] <- 'KERENDANG'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'PALMERIAM'] <- 'PAL MERIAM'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'PINANGRANTI'] <- 'PINANG RANTI'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'RAWAJATI'] <- 'RAWA JATI'\n\n\n\n\n5.4 Joining + EDA\nNow, we have a standardised common identifier among our geospatial and aspatial dataframes. Let’s join them once more:\n\n\nShow the code\n# joins cases_jakarta to bd_jakarta based on Sub_District\ncombined_jakarta <- left_join(bd_jakarta, cases_jakarta,\n                              by=c(\n                                \"Sub_District\"=\"Sub_District\")\n                              )\n\n\nNow, let’s once again visualise our updated combined_jakarta in terms of Target:\n\n\nShow the code\nupdated_target = tm_shape(combined_jakarta)+\n  tm_fill(\"Target\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Updated Target\")\nupdated_target\n\n\n\n\n\n\n\n5.5 Calculations\nBefore we move into into EDA and thematic mapping, we need to calculate the monthly vaccination rate, as per assignment requirements.\n\n5.5.1 Monthly Vaccination Rate\nThe monthly vaccination rate can be calculated by minusing To_Be_Vaccinated from Target, then diving it by Target. This would be based on the sub-district and date.\n\n\nShow the code\n# grouping based on the sub-district and date\nMVR <- cases_jakarta %>%\n  inner_join(bd_jakarta, by=c(\"Sub_District\" = \"Sub_District\")) %>%\n  group_by(Sub_District, Date) %>%\n  dplyr::summarise(`monthly_vaccinated_rate` = (((Target) - (To_Be_Vaccinated))/Target)) %>%\n  \n  #afterwards, pivots the table based on the Dates, using the monthly vaccinated rate as the values\n  ungroup() %>% pivot_wider(names_from = Date,\n              values_from = monthly_vaccinated_rate)\n\n\nOur MVR should look like this:\n\n\n\n\n5.6 Converting dataframes of sf objects\nBefore we continue to mapping, we should convert these dataframes into sf objects.\n\n\nShow the code\ncombined_jakarta <- st_as_sf(combined_jakarta)\n\n# need to join our previous dataframes with the geospatial data to ensure that geometry column is present\nMVR <- MVR%>% left_join(bd_jakarta, by=c(\"Sub_District\"=\"Sub_District\"))\nMVR <- st_as_sf(MVR)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mapping-monthly-vaccination-rate",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mapping-monthly-vaccination-rate",
    "title": "Take-home Exercise 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "6.0 Mapping: Monthly Vaccination Rate",
    "text": "6.0 Mapping: Monthly Vaccination Rate\n\n6.1 Jenks Choropleth Map\nThe Jenks method clusters data into groups that minimize the within-group variance and maximize the between-group variance (source). However, it will not work as well if the data has low variance, so let’s check the variance.\n\nvar(MVR$`2021-07-01`)\n\n[1] 0.003634884\n\n\n\nvar(MVR$`2022-06-01`)\n\n[1] 0.0002652066\n\n\nSince the variance is very low, Jenks would not be suitable. Let’s try another classification method.\n\n\n6.2 Quantile Choropleth Map\nQuantile classification is a data classification method that distributes a set of values into groups that contain an equal number of values. The attribute values are added up, then divided into the predetermined number of classes (source).\nAfter playing around with different class numbers, 6 classes seems to be just nice. If there are too many classes, it becomes hard to read and difficult to understand the differences. If there are too few classes, you might not be able to see any differences.\n\n\nShow the code\nlibrary(tmap)\n\n# using the jenks method, with 6 classes\ntmap_mode(\"plot\")\ntm_shape(MVR)+\n  tm_fill(\"2021-07-01\", \n          n= 6,\n          style = \"quantile\", \n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Vaccination Rate in July 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.5, \n            legend.width = 0.4,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNow, we will plot it for all months with a helper function.\n\n\nShow the code\n# input: the dataframe and the variable name - in this case, the month \n# with style=\"quantile\" for the quantile classification method\nquantile_plot <- function(df, varname) {\n  tm_shape(MVR) +\n    tm_polygons() +\n  tm_shape(df) +\n    tm_fill(varname, \n          n= 6,\n          style = \"quantile\", \n          title = \"Vaccination Rate\") +\n    tm_layout(main.title = varname,\n          main.title.position = \"center\",\n          main.title.size = 1.2,\n          legend.height = 0.45, \n          legend.width = 0.35,\n          frame = TRUE) +\n    tm_borders(alpha = 0.5)\n}\n\n\nLet’s visualise the quantile plots for all months.\n\n\nShow the code\n# split it up into multiple arranges to make it easier to see\nlibrary(tmap)\ntmap_mode(\"plot\")\ntmap_arrange(quantile_plot(MVR, \"2021-07-01\"),\n             quantile_plot(MVR, \"2021-08-01\"),\n             quantile_plot(MVR, \"2021-09-01\"),\n             quantile_plot(MVR, \"2021-10-01\"))\n\n\n\n\n\n\n\nShow the code\ntmap_arrange(quantile_plot(MVR, \"2021-11-01\"),\n             quantile_plot(MVR, \"2021-12-01\"),\n             quantile_plot(MVR, \"2022-01-01\"),\n             quantile_plot(MVR, \"2022-02-01\"))\n\n\n\n\n\n\n\nShow the code\ntmap_arrange(quantile_plot(MVR, \"2022-03-01\"),\n             quantile_plot(MVR, \"2022-04-01\"),\n             quantile_plot(MVR, \"2022-05-01\"),\n             quantile_plot(MVR, \"2022-06-01\"))\n\n\n\n\n\n\n\n6.3 Observations from Quantile Choropleth Map\nDo note that each map has its own relative case rate: the ranges gradually grow larger over time with the greater influx of vaccination counts. By comparing the change of vaccination rates over the months, there are a number of observations we can make:\n\nFrom July 2021 to October 2021, vaccination rates for certain sub-districts in the West and Central of Jakarta were relatively high.\nFrom November 2021 to February 2022, vaccination rates for most of the sub-districts in the South of Jakarta were relatively high.\nFrom March 2022 to June 2022, vaccination rates for the sub-districts that were relatively high since November 2021 continued to remain high until June 2022.\nBy analysis the maps by thirds, we can tell which sub-districts were pro-vaccination and anti-vaccination based on the vaccination rates.\n\nLet us check for the sub-districts with the highest vaccination rates at various stages.\n\n\nShow the code\n# to check for darkest sub-district in early stages\nMVR$Sub_District[which.max(MVR$`2021-07-01`)]\n\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\n\n\n\nShow the code\n# to check for darkest sub-district in middle stages\nMVR$Sub_District[which.max(MVR$`2022-01-01`)]\n\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\n\n\n\nShow the code\n# to check for darkest sub-district in later stages\nMVR$Sub_District[which.max(MVR$`2022-06-01`)]\n\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\n\n\n\nShow the code\n# reorder based on decreasing values for Jan 2022, and assign to a new df\nMVR_descending <- MVR[order(MVR$`2022-01-01`,decreasing = TRUE),]\nMVR_descending_top5 <- MVR_descending[1:5,]\nMVR_descending_top5[,\"Sub_District\"]\n\n\nSimple feature collection with 5 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -3627698 ymin: 664154 xmax: -3613476 ymax: 693390.8\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\n# A tibble: 5 x 2\n  Sub_District                                                          geometry\n  <chr>                                                       <MULTIPOLYGON [m]>\n1 HALIM PERDANA KUSUMAH (((-3616607 680982.6, -3616583 680983, -3616519 680987.~\n2 SRENGSENG SAWAH       (((-3622581 666961.1, -3622554 666932.7, -3622512 66691~\n3 GLODOK                (((-3627130 693207.2, -3627121 693187.5, -3627115 69316~\n4 KELAPA GADING TIMUR   (((-3614674 692511.7, -3614646 692504.4, -3614627 69250~\n5 MANGGARAI SELATAN     (((-3621502 683539.5, -3621482 683545.2, -3621481 68352~\n\n\nFrom this, we can tell that HALIM PERDANA KUSUMAH was the most pro-vaccination sub-district in Jakarta,, followed by SREMGSEMG SAWAH, GLODOK, KELAPA GADING TIMUR and MANGGARI SELATAN."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-measures-of-spatial-association---sfdep-methods",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-measures-of-spatial-association---sfdep-methods",
    "title": "Take-home Exercise 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "7.0 Local Measures of Spatial Association - sfdep methods",
    "text": "7.0 Local Measures of Spatial Association - sfdep methods\nAccording to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”\n\n7.1 Hot Spot and Cold Spot Area Analysis (HCSA)\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure. If the value is high, it is a hot spot, vice versa.\n\nComputing local Gi* statistics\n\n\nShow the code\nwm_idw <- MVR %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\n\n\nShow the code\nHCSA <- wm_idw %>% \n  mutate(local_Gi = local_gstar_perm(\n    `2021-07-01`, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\nHCSA\n\n\nSimple feature collection with 261 features and 31 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -3644275 ymin: 663887.8 xmax: -3606237 ymax: 701380.1\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\n# A tibble: 261 x 32\n   gi_star    e_gi      var_gi p_value p_sim p_fol~1 skewn~2 kurto~3 nb    wts  \n     <dbl>   <dbl>       <dbl>   <dbl> <dbl>   <dbl>   <dbl>   <dbl> <nb>  <lis>\n 1  1.78   0.00385     4.43e-8 0.0747   0.06    0.03 -0.198  -0.587  <int> <dbl>\n 2 -1.13   0.00381     7.31e-8 0.258    0.3     0.15 -0.0354  0.0558 <int> <dbl>\n 3 -2.58   0.00385     7.47e-8 0.00987  0.04    0.02  0.151  -0.0259 <int> <dbl>\n 4 -2.26   0.00388     6.74e-8 0.0237   0.02    0.01  0.465  -0.289  <int> <dbl>\n 5 -1.03   0.00386     8.17e-8 0.302    0.28    0.14  0.322  -0.320  <int> <dbl>\n 6  0.773  0.00381     3.60e-8 0.439    0.38    0.19  0.235   0.625  <int> <dbl>\n 7 -0.0137 0.00379     8.85e-8 0.989    0.98    0.49  0.122  -0.0205 <int> <dbl>\n 8 -2.61   0.00386     7.13e-8 0.00904  0.02    0.01  0.439  -0.413  <int> <dbl>\n 9 -0.385  0.00379     5.57e-8 0.700    0.74    0.37 -0.0403  0.205  <int> <dbl>\n10 -2.45   0.00387     6.19e-8 0.0144   0.02    0.01  0.0480 -0.411  <int> <dbl>\n# ... with 251 more rows, 22 more variables: Sub_District <chr>,\n#   `2021-07-01` <dbl>, `2021-08-01` <dbl>, `2021-09-01` <dbl>,\n#   `2021-10-01` <dbl>, `2021-11-01` <dbl>, `2021-12-01` <dbl>,\n#   `2022-01-01` <dbl>, `2022-02-01` <dbl>, `2022-03-01` <dbl>,\n#   `2022-04-01` <dbl>, `2022-05-01` <dbl>, `2022-06-01` <dbl>,\n#   Object_ID <dbl>, Village_Code <chr>, Village <chr>, Code <dbl>,\n#   Province <chr>, City <chr>, District <chr>, Total_Population <dbl>, ...\n\n\n\n\n\n7.2 Visualisng Hot Spot and Cold Spot Areas\nNow, we are ready to plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.\n\n\nShow the code\nHCSA_sig <- HCSA  %>%\n  filter(p_sim < 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\",\n          n=5) + \n  tm_borders(alpha = 0.4) +\ntm_layout(main.title = \"GI* Vaccination Rate in July 2021\",\n          main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.5, \n            legend.width = 0.4,\n            frame = TRUE)\n\n\n\n\n\nNow, we will plot it for all months with a helper function.\n\n\nShow the code\ngi_star_plot <- function(varname, varname_two) {\n  HCSA <- wm_idw %>% \n  mutate(local_Gi = local_gstar_perm(\n    varname, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n  \n  HCSA_sig <- HCSA  %>%\n  filter(p_sim < 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\",\n          n=5) + \n  tm_borders(alpha = 0.4) +\ntm_layout(main.title = varname_two,\n          main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.5, \n            legend.width = 0.4,\n            frame = TRUE)\n}\n\n\nLet’s visualise the GI* plots for all months.\n\n\nShow the code\n# split it up into multiple arranges to make it easier to see\nlibrary(tmap)\ntmap_mode(\"plot\")\ntmap_arrange(gi_star_plot(HCSA$`2021-07-01`, \"2021-07-01\"),\n             gi_star_plot(HCSA$`2021-08-01`, \"2021-08-01\"),\n             gi_star_plot(HCSA$`2021-09-01`, \"2021-09-01\"),\n             gi_star_plot(HCSA$`2021-10-01`, \"2021-09-01\"))\n\n\n\n\n\n\n\nShow the code\ntmap_arrange(gi_star_plot(HCSA$`2021-11-01`, \"2021-11-01\"),\n             gi_star_plot(HCSA$`2021-12-01`, \"2021-12-01\"),\n             gi_star_plot(HCSA$`2022-01-01`, \"2022-01-01\"),\n             gi_star_plot(HCSA$`2022-02-01`, \"2022-02-01\"))\n\n\n\n\n\n\n\nShow the code\ntmap_arrange(gi_star_plot(HCSA$`2022-03-01`, \"2022-03-01\"),\n             gi_star_plot(HCSA$`2022-04-01`, \"2022-04-01\"),\n             gi_star_plot(HCSA$`2022-05-01`, \"2022-05-01\"),\n             gi_star_plot(HCSA$`2022-06-01`, \"2022-06-01\"))\n\n\n\n\n\n\n\n7.3 Observations from Gi* Maps\nLooking at the placements of hot spots (green) and cold spots (yellow/orange), it coincides with the Quantile Choropleth maps that showed the sub-district which were pro-vaccination and anti-vaccination."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#emerging-hot-spot-analysis-ehsa-sfdep-methods",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#emerging-hot-spot-analysis-ehsa-sfdep-methods",
    "title": "Take-home Exercise 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "8.0 Emerging Hot Spot Analysis (EHSA): sfdep methods",
    "text": "8.0 Emerging Hot Spot Analysis (EHSA): sfdep methods\nEmerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\n\nBuilding a space-time cube,\nCalculating Getis-Ord local Gi* statistic for each bin by using an FDR correction,\nEvaluating these hot and cold spot trends by using Mann-Kendall trend test,\nCategorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.\n\n\n8.1 Creating a Time Series Cube\nIn the code chunk below, spacetime() of sfdep is used to create an spatio-temporal cube.\n\n\nShow the code\nHCSA_st <- spacetime(cases_jakarta, bd_jakarta,\n                      .loc_col = \"Sub_District\",\n                      .time_col = \"Year\")\n\n\nNext, is_spacetime_cube() of sfdep package will be used to verify if GDPPC_st is indeed an space-time cube object\n\nis_spacetime_cube(HCSA_st)\n\n[1] FALSE"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "",
    "text": "Housing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.\nConventional, housing resale prices predictive models were built by using Ordinary Least Square (OLS) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, Geographical Weighted Models were introduced for calibrating predictive model for housing resale prices.\n\n\n\nTo predict HDB resale prices at the sub-market level (i.e. HDB 3-room, HDB 4-room and HDB 5-room) for the month of January and February 2023 in Singapore. The predictive models must be built by using by using conventional OLS method and GWR methods. You are also required to compare the performance of the conventional OLS method versus the geographical weighted methods.\n\n\n\n\n\nFor the purpose of this take-home exercise, HDB Resale Flat Prices provided by Data.gov.sg should be used as the core data set. The study should focus on either three-room, four-room or five-room flat and transaction period should be from 1st January 2021 to 31st December 2022. The test data should be January and February 2023 resale prices.\n\n\n\nFor the spatial data, we will be using the Master Plan 2019 Subzone Boundary (No Sea), which can be downloaded from data.gov."
  }
]